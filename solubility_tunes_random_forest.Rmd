---
title: "solubility_tuned_random_forest.Rmd"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(ranger)
library(usemodels)
library(gridExtra)
theme_set(theme_classic())
```

Aqueous solubility (ability to dissolve in water) is an essential property of a chemical compound important in the laboratory. Can the solubility of a compound be predicted based on a chemical structure alone? John Delaney posed this predictions question in 2004 [(Delaney 2004)](https://pubs.acs.org/doi/10.1021/ci034243x) and wrote a paper with numerous citations in the chemistry literature. This study will take a dataset similar to that study and use linear and random forest regression to predict the compounds' solubilities. *The random forest model is a much better predictor of solubilities*.

A number of compounds in this dataset are well-known, even outside the chemistry community. Here is a sample of what lies inside the dataset:

##### Table 1: Well-known compounds in the dataset
| Compound name | Description |
|---|---|
[Sucrose](https://en.wikipedia.org/wiki/Sugar) | Sugar
[Erythritol](https://en.wikipedia.org/wiki/Erythritol) | Sugar substitute
[Caffiene](https://en.wikipedia.org/wiki/Caffeine) | Coffee time!
[Fructose](https://en.wikipedia.org/wiki/Fructose) | Component of high fructose corn syrup

##  Dataset Description

The original report published a dataset of compounds represented as SMILES strings. SMILES strings are a compact and text-based method of specifying chemical structures. This study will use a preprocessed dataset from deepchem.io, which contains features parsed from these SMILES strings. [You can browse the file on GitHub.](https://github.com/deepchem/deepchem/blob/master/datasets/delaney-processed.csv)This study uses a subset of these preprocessed features, which are listed in Table 2. 

##### Table 2: Features of each compound used in the regression 

| Feature name | Units | Description
|---|---|---|
`mw` | g/mol | The molecular weight of the compound.
`solubility` | log(mol/L) | The log solubility, in mol/L. Solubility is the response variable of this study.
`psa` | Ã…<sup>2</sup> | The polar surface area of a molecule.
`h_bond_donors` | unitless | The number of hydrogen bond donors on a molecule.<sup>1</sup>
`rotatable_bonds` | unitless | The number of rotatable bonds within a molecule.<sup>2</sup>

## Complete source code

To make this blog post easier to read, I have omitted the source code. However, [you can read the source code in it entirety on GitHub.](https://github.com/akey7/solubility-ml/blob/v0.0.2/solubility_regression.Rmd)

## Exploratory visualization

``` {r include = FALSE}
df <- as_tibble(read.csv("data/delaney-processed.csv")) %>%
  select(
    compound = Compound.ID, 
    mw = Molecular.Weight, 
    h_bond_donors = Number.of.H.Bond.Donors, 
    rings = Number.of.Rings, 
    rotatable_bonds = Number.of.Rotatable.Bonds, 
    psa = Polar.Surface.Area, 
    solubility = measured.log.solubility.in.mols.per.litre
)
```

## Split and bootstrap sample the data

```{r}
split <- initial_split(df)
train_data <- training(split)
test_data <- testing(split)
train_boot <- bootstraps(train_data)
```

## Implement the suggestion

```{r}
ranger_recipe <- 
  recipe(formula = solubility ~ mw + h_bond_donors + rings + rotatable_bonds + 
    psa, data = train_data) 

ranger_spec <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 2000) %>% 
  set_mode("regression") %>% 
  set_engine("ranger") 

ranger_workflow <- 
  workflow() %>% 
  add_recipe(ranger_recipe) %>% 
  add_model(ranger_spec) 

set.seed(86881)
doParallel::registerDoParallel()
ranger_tune <-
  tune_grid(ranger_workflow, resamples = train_boot, grid = 30)
```

## Explore the tuning results

### Show the values of the best random forests

```{r}
autoplot(ranger_tune) +
  theme_grey()
```

### Finalize the workflow on the best forest

```{r}
final_rf <- ranger_workflow %>%
  finalize_workflow(select_best(ranger_tune, metric = "rmse"))
```

## Predictions and predictor importance

```{r}
solubilty_fit <- last_fit(final_rf, split)
collect_metrics(solubilty_fit)
```

```{r}
collect_predictions(solubilty_fit) %>%
  ggplot(aes(.pred, solubility)) +
  geom_point(alpha = 0.4) +
  stat_smooth(method = "lm", se = FALSE)
```

## Results

The untuned random forest using ranger has an RMSE = 0.866 and R^2 = 0.828. This tuned random forest edges out these scores with RMSE = 0.840 and R^2 = 0.830. Most of the benefit from 2000 tuned trees (as opposed 500 untuned trees) came in the decrease of RMSE. However, this random forest was computationally expensive, whereas the untuned model was not. The untuned model may have the best computational complexity and performance tradeoff. 

